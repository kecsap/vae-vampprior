{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple-vae-sb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMm2s9k8G35IaIhGq3mnU1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morpheusthewhite/vae-vampprior/blob/main/notebooks/simple_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaMbF7MeYdhp"
      },
      "source": [
        "# A \"Hello world\" Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw3OCaoKYmlZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FahNh0awY0kZ"
      },
      "source": [
        "Let's start by loading the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLiZDFAAZBl8"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# map to float values\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8m0po3DbVFf"
      },
      "source": [
        "In our case the `y` will correspond to the `x` itself. But we need to flatten it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzNVpYmRbsgT"
      },
      "source": [
        "flattened_size = x_train[0].shape[0] * x_train[0].shape[1]\n",
        "\n",
        "y_train = np.reshape(x_train, (-1, flattened_size))\n",
        "y_test = np.reshape(x_test, (-1, flattened_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDjwu9taO88m"
      },
      "source": [
        "# utility for visualizing the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def vae_show_results(model, sample):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  \n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(sample)\n",
        "  \n",
        "  # feed data to the autoencoder\n",
        "  result = model(np.array([sample]))[0]\n",
        "  \n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECJ7bP4iZm0l"
      },
      "source": [
        "We can initially try with a very simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezdhiD8jE4k3"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, K, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.K = K\n",
        "\n",
        "  def build(self, inputs_shape):\n",
        "    self.flatten = layers.Flatten(input_shape=(inputs_shape[1], inputs_shape[2]),\n",
        "                                               name='enc-flatten')\n",
        "\n",
        "    self.dense0 = layers.Dense(300, name='enc-dense0', activation='relu')\n",
        "    self.dense1 = layers.Dense(300, name='enc-dense1', activation='relu')\n",
        "\n",
        "    self.dense_mu = layers.Dense(self.K, name='enc-out-mu')\n",
        "    self.dense_logvar = layers.Dense(self.K, name='enc-out-lo')\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    flattened = self.flatten(inputs)\n",
        "\n",
        "    x = self.dense0(flattened)\n",
        "    x = self.dense1(x)\n",
        "\n",
        "    mu = self.dense_mu(x)\n",
        "    logvar = self.dense_logvar(x)\n",
        "    return mu, logvar\n",
        "  \n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def __init__(self, K, L, **kwargs):\n",
        "    super(Sampling, self).__init__(**kwargs)\n",
        "    self.L = L\n",
        "    self.K = K\n",
        "    \n",
        "    self.normal_standard = tfp.distributions.MultivariateNormalDiag(\n",
        "                              tf.zeros(shape=(self.K,)), \n",
        "                              tf.ones(shape=(self.K,)))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    mu, logvar = inputs\n",
        "    latent_samples = \\\n",
        "        self.normal_standard.sample((self.L, mu.shape[0])) * tf.sqrt(tf.exp(logvar)) + mu\n",
        "      \n",
        "    # the returned samples will have shape (N, L, K)\n",
        "    # where N is the size of the batch\n",
        "    return tf.reshape(latent_samples, (-1, self.L, self.K))\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_shape, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.output_shape_ = output_shape\n",
        "\n",
        "    self.dense0 = layers.Dense(300, name='dec-dense0', activation='relu')\n",
        "    self.dense1 = layers.Dense(300, name='dec-dense1', activation='relu')\n",
        "    self.reconstruct = layers.Dense(output_shape[0] * output_shape[1], name='dec-out')\n",
        "\n",
        "  def build(self, inputs_shape):\n",
        "    # transform the result into a square matrix\n",
        "    # the result of a single input will be a (L, M, M) tensor\n",
        "    # where M is the size of the original image\n",
        "    self.reshape = layers.Reshape((inputs_shape[1],\n",
        "                                   self.output_shape_[0], self.output_shape_[1]),\n",
        "                                  name='dec-out-reshaped')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # inputs will have shape (N, L, K)\n",
        "    x = self.dense0(inputs)\n",
        "    x = self.dense1(x)\n",
        "\n",
        "    x = self.reconstruct(x)\n",
        "\n",
        "    # once reshaped it will have shape (N, L, M, M)\n",
        "    return self.reshape(x)\n",
        "\n",
        "\n",
        "# reduce on mean along the L components\n",
        "class MeanReducer(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MeanReducer, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # inputs has shape (N, L, M, M)\n",
        "    # output has shape (N, M, M)\n",
        "    return tf.reduce_mean(inputs, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWrsDCVHpt5-"
      },
      "source": [
        "class VAE(tf.keras.Model):\n",
        "  def __init__(self, K, L, **kwargs):\n",
        "    super(VAE, self).__init__(**kwargs)\n",
        "    self.K = K\n",
        "    self.L = L\n",
        "\n",
        "  def build(self, inputs_shape):\n",
        "    self.encoder = Encoder(self.K)\n",
        "    self.sampling = Sampling(self.K, self.L)\n",
        "    self.decoder = Decoder((inputs_shape[1], inputs_shape[2]))\n",
        "    self.mean_reducer = MeanReducer()\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    mu, logvar = self.encoder(inputs)\n",
        "    sigma = tf.sqrt(tf.exp(logvar))\n",
        "    samples = self.sampling((mu, logvar))\n",
        "\n",
        "    # TODO: improve numerical stability and remove epsilon\n",
        "    #   to improve decoding performances\n",
        "    \n",
        "    # epsilon for avoiding log explosion in loss\n",
        "    eps = 1e-18                  \n",
        "    \n",
        "    # loss due to regularization \n",
        "    # first addend, corresponding to log( p_lambda (z_phi^l) )\n",
        "    normal_standard = tfp.distributions.MultivariateNormalDiag(tf.zeros(self.K), tf.ones(self.K))\n",
        "    log_p_lambda = tf.math.log(eps + normal_standard.prob(samples))\n",
        "    \n",
        "    # second addend, corresponding to log( q_phi (z|x) )\n",
        "    # where q_phi=N(z| mu_phi(x), sigma^2_phi(x)) \n",
        "    normal_latent = tfp.distributions.MultivariateNormalDiag(mu, sigma)\n",
        "    log_q_phi = tf.math.log(eps + normal_latent.prob(samples))\n",
        "    \n",
        "    regularization_loss = tf.math.subtract(tf.math.reduce_mean(log_q_phi),\n",
        "                                          tf.math.reduce_mean(log_p_lambda),\n",
        "                                          name='regularization-loss')\n",
        "    self.add_loss(regularization_loss)\n",
        "\n",
        "    reconstructed = self.decoder(samples)\n",
        "\n",
        "    return self.mean_reducer(reconstructed)\n",
        "    # return reconstructed\n",
        "\n",
        "  def decode(self, inputs):\n",
        "    # inputs will have shape (N, K)\n",
        "    reconstructed = self.decoder(inputs)\n",
        "\n",
        "    return reconstructed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_suZgTrKdW"
      },
      "source": [
        "vae = VAE(40, 1)\n",
        "vae.compile(optimizer='adam', loss=tf.nn.sigmoid_cross_entropy_with_logits)\n",
        "vae.fit(x_train, x_train, epochs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRvNKAJpgFEW"
      },
      "source": [
        "samples = x_train[1:5]\n",
        "\n",
        "for sample in samples:\n",
        "  vae_show_results(vae, sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZyULkzRhq3A"
      },
      "source": [
        "samples = x_test[1:5]\n",
        "\n",
        "for sample in samples:\n",
        "  vae_show_results(vae, sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjsd1A2fSxG4"
      },
      "source": [
        "What if we try to generate some data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KokqVSJDSz-9"
      },
      "source": [
        "def vae_show_generated(generated):\n",
        "\n",
        "  for image in generated:\n",
        "    plt.figure();\n",
        "    plt.imshow(image[0])\n",
        "\n",
        "def vae_generate(vae, n, K):\n",
        "  samples = np.random.normal(0, 1, size=(n, K))\n",
        "  return vae.decode(samples)\n",
        "\n",
        "generated_data = vae_generate(vae, 5, 40)\n",
        "vae_show_generated(generated_data)\n",
        "generated_data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}